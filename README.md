# airflow_docker
![image](https://github.com/user-attachments/assets/673ebfc9-766e-43fe-9bae-bde98b570d74)

# Project Objective
- Design and develop a data pipeline for batch processing the city of Chicago Traffic incidents
- Develop analytical views and dashboard with the extracted data
- Perform Data transformation with Pandas and Pyspark
- Store the traffic data into data warehouse and data lakes
- Develop data models, facts and dimension tables with dbt
- Dataset link 1: https://data.cityofchicago.org/Transportation/Traffic-Crashes-Vehicles/68nd-jvt3/about_data
- Dataset link 2: https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if/about_data
- Dataset link 3: https://data.cityofchicago.org/Transportation/Traffic-Crashes-People/u6pd-qa9d/about_data

# Technologies
- Workflow Orchestration: Apache Airflow
- Data Warehouse: Big Query
- Data Lake: Google Cloud Storage
- Data Visualization: Looker Studio
- Data Modeling: dbt
- Containerization: Docker
- Batch Processing: Spark
- Google Cloud Services: DataProc
